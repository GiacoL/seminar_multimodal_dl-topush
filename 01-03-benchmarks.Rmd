---
output:
  pdf_document: default
---
# Resources and Benchmarks for NLP, CV and multimodal tasks

*Author: Christopher Marquardt*

*Supervisor: Prof. Dr. Christian Heumann*

When  see athletes perform in their sports  only see the results of their hard work prior or till to the event. Most of the time they casually talk about their off-season, but everybody knows the results are made in the off-season. 

Same goes for the models  will see in the later chapters. Most of the time  are just interested in our results, but why and how does the model come to these results? It has to learn to some key fundamentals of the modality to achieve these results. But how do  get them to perform in such a way or even better?  can build better architectures and/or use more and new data. New data by hand is easy to get but this new data results in a new problem. New data has to be carefully labeled by humans, which can be very expensive by the amount of data. Models which learn from labeled data use the supervised learning strategy. This learning strategy is a bottleneck for future progress.

But the need for labeling the data isn't the only problem. Let's visit the athlete analogy again. Imagine a professional football player has to participate in a professional ski race. He will not be able to compete with the others, because they are trained only to do ski races. Here  see the other problem. Models which use supervised learning have shown to perform very ll on the task they re trained to do. This means models which learn on carefully labeled data only perform very ll on this specific task, but poor on others. Also it's not possible to label everything in the world. 

So the goal is to generate more generalist models which can perform ll on different tasks without the need of huge labeled data. Humans are able to perform ll on different tasks in a short amount of time. For example humans only need a small amount of hours to learn how to drive a car, even without supervision. On the other hand fully automated driving AI need thousand of hours of data to drive a car. Why do humans learn so fast compared to machines?
Humans don't rely on labeled data, because most of the time humans learn by observation. By this humans generate a basic knowledge of how the world works, which also called common sense. This enables us to learn so much faster compared to machines.
Meta AI (cite bpage) believes that self-supervised learning is one of the most promising ways to generate background knowledge and some sort of common sense in AI systems. By Self-supervised learning one means a supervised learning algorithm, but it doesn't need an external supervisor.  will see later how the different modalities use this approach, as it varies beten the modalities.

Hier fehlt noch der Ãœbergang

The following chapter will inspect on the one hand pre-training resources and the use of them and on the other hand also the benchmarks which are used for NLP, CV and ,the combination of both, multi-modal learning.

\subsection{Pre-training}
  
After pointing out that pre-training is very important one might ask how do the different modalities pre-train and how do the datasets look? At first  will inspect the last one and focus afterwards on the use of the resources.
As one might expect NLP models pre-train on text, CV models pre-train on images and MML models pre-train on text image pairs, which can somehow be seen as a combination of NLP and CV. But CV models mostly use labeled data like a picture of a dog with the corresponding label "dog". MML datasets can contain several sentences of text which correspond to the image.

Even if the datasets might be completely different, the procedure to get the data is the same for all of them, because the data is downloaded from the internet. This can lead to a problem, since by using this method the resulting dataset might be noisy. One approach for the MML models is to use common crawl and extract the image plus the alt of a image. The alt is an alternate text for an image, if the image cannot be displayed or for visual impaired people. This seems like a reasonable approach, but the alt is often not very informative about what's in the image. (maybe add example)

The next difference beten the modalities is the cardinality of the pre-training data. It's easy to see that text is by far the most easy to get from the internet. This results in huge high-quality massive text data. Some magnitudes smaller are the datasets for CV. Since MML is pretty new compared to the other modalities it still relatively small but growing fast. A small downer is that some of the datasets are not public available. The big companies like to keep their models and used datasets private, which hinders the reproducibility, but there are also real open AI competitors like LAION in the field. 
The next chapter will provide some of the most used pre-training datasets.

\subsubsection{Resources for pre-training}

The first modality will be NLP, then CV and at last MML. Only three datasets per modality will be provided, but resources for more will be at the end of the chapter.
As already mentioned, extracting text from the internet is rather easy. More precisely there is a non-profit organization, called [Common Crawl](https://commoncrawl.org), which does exactly this. They provide copies of the internet to researchers, companies and individuals at no cost for the purpose of research and analysis. The Common Crawl corpus contains petabytes of data collected since 2008. It contains raw b page data, extracted metadata and text extractions. The advantages of Common Crawl come along with their disadvantages. The text is from diverse domains but with varying quality of data. To handle the raw nature of the datasets one often has to use a ll-designed extraction and filter to use the datasets appropriately [@gao2020pile]. For example GPT-3 uses a filtered version of Common Crawl, which consists of 410 billion tokens [@brown2020language].
But recent work [@rosset2020turing] shod that diversity in training dataset improves general cross-domain knowledge and downstream generalization capability for language models. The Pile [@gao2020pile] was introduced to address exactly these results. The Pile contains $22$ sub-datasets, including established NLP datasets, but also several newly introduced ones. The size of the $22$ sub-datasets, which can be categorized into 5 categories, pile up to around $825$ GB of data.
The following treemap shows the distribution of the dataset.

```{r pile treemap, echo=FALSE}
htmltools::includeHTML("data/01-chapter1/CompositionPile.html")
# Plot passt leider noch nciht ganz. Aus einem Grund kommt eine Category nicht obwohl sie im .html sonst gezeigt wird
# label
```

A more detailed description of the Pile can be found in the corresponding paper [@gao2020pile].
The last dataset for NLP is the BooksCorpus dataset [@zhu2015aligning]. The BooksCorpus uses books from yet unplished authors from the b. Only books with more than 20k words re included to filter out shorter, noisier stories.
This results in around 11k books from 16 different genres. So more than 74 million sentences can be used in pre-training.

The next inspected modality is CV. Almost every state-of-the-art CV model uses a classifier pre-trained on an ImageNet based dataset. ImageNet uses the hierarchical structure of WordNet [@fellbaum2010wordnet]. At the release of ImageNet-1k the amount of classes was unheard at this time point. Datasets like CIFAR-10 [@krizhevsky2009learning] and CIFAR-100 [@krizhevsky2009learning] had 10 and 100 classes, but ImageNet1k had 1000 different classes and this was not the only major improvement. They also increased the resolution from $32 \times 32$ to $256 \times 256$.  In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. The ImageNet-1k dataset is a subset of the ImageNet dataset [@deng2009imagenet]. The full ImageNet dataset is also called ImageNet-21k. It consists of more than 14 million images, divided in almost 22k classes. Because of this some paper described it as ImageNet-22k. Those two dataset do not only differ by the amount of classes, but also by the type of labels. The labels of ImageNet-21k are not mutually exclusive. Because of this the pre-training wiht ImageNet-1k is far more popular. Also the ImageNet-21k dataset lacks an official train-validation split., which is just another reason why ImageNet-1k is more popular. The raw dataset ImageNet-21k is around 1.3 terabyte (TB). It's also nice, that the the dataset of ImageNet are open available. The next dataset is in contrast to this, because it's no available.

The Joint-Foto-Tree (JFT) 300M is one of the follow up version of the JFT dataset [@hinton2015distilling]. Given the name it consists of 300 million images and on average each image has 1.26 labels. The whole datasets has around 375 million labels. These labels can be divided into 18291 classes. These categories form a rich hierarchy with the maximum depth of hierarchy being 12 and maximum number of child for parent node being 2876 [@sun2017revisiting]. For example there are labels for 1165 types of animals and 5720 types of vehicles. The work states that approximately 20% of the labels in this dataset are noisy [@sun2017revisiting], because the labels are generated automatically. It also provides the fact, that the distribution is heavily long-tailed, which means that some of the classes have less than 100 images. There is also an extendend version of the JFT dataset. It's called Ensamble Foto Tree (EFT) and consists of 100k classes, but it's rarely used in practice because of the intolerable large model size and the slow training speed.

The Pile is an attempt to mimic the dataset used for GPT-3 and LAION wants to achieve the same. LAION-400M [@schuhmann2021laion] consists of 400 million image-text pairs. These 400 million image-text pairs were filtered with CLIP. The dataset also contains the CLIP embedding and kNN indices. @schuhmann2021laion describes the procedure to create the dataset in an open manner. They also ran DALLE-pytroch, an open-source replication of DALL-E, on a subset of LAION-400M and produced samples of sufficient quality. This opens the road for large-scale training and research of language-vision models, which was previously not possible.


MMl
\begin{itemize}
  \item LAION 400M & 5B
  \item CC
  \item COCO
\end{itemize}

   \begin{itemize}
    \item provide resources to find more (papers with code, ...)
  \end{itemize}
    
  \subsubsection{Use of resources}
    \begin{itemize}
      \item How pre-training is used in the modalities
      \item How has pre-training changed or has it even changed in modalities
        \begin{itemize}
          \item supervised; just tell: not used anymore and skip to self-supervised
          \item self-supervised; cite that SSL is more similiar tp the learning of a human then supervised (Yann le , yannic kilcher interview)
          \item BERT, GPT-3 use MLM, NSP; NSP not really needed (deBERTa or RoBERTa)
          \item easy in NLP but not easy in CV, 
          \item Solution space in NLP is finite in CV not (The dark matter of intelligence blog)
          \item contrastive loss => need for hard negatives (bad)
          \item BYOL approach, gets rid of hard negatives => good
          \item Self-distillation (student, teacher model)
          \item augmentation used to mimic MLM
        \end{itemize}
        \item bla
         \begin{itemize}
        \item CV: still all train on ImageNet ("Are  done with ImageNet") and poor performance of ObjectNet
            \item use of noisy data; ?quantity > quality?
        \end{itemize}
        \item State and explain 2 or 3 main used pre-training tasks
          \begin{itemize}
            \item masked approaches
            \item ...
          \end{itemize}
      \end{itemize}
    
\subsection{Fine-tuning}
  \begin{itemize}
    \item why fine-tuning is important
    \item where and how to fine-tune
    \item use labeled data for down-stream tasks
    \item ... 
  \end{itemize}

\subsection{Benchmarks for modalities}
  \begin{itemize}
    \item Importance of benchmarks
      \begin{itemize}
        \item also need for new ones (like Psych: Flynn Effect)
          \begin{itemize}
            \item do models get better or is it possible that pre-training                         contains already benchmarks (too much crawl; NLP)
          \end{itemize}
        \item Hint that models pre-train on different resources but perform on same benchmarks (good or bad)
        \end{itemize}
      \item different tasks for benchmarks
        \begin{itemize}
          \item state most important ones also give infor to find ohters (example: papers with code)
          \item Hint that it's most of the time reduction to classification tasks (like is this next sentences)
          \item Semantic of produced sentences often not nice
        \end{itemize}
  \end{itemize}


Outro: Multimodal architectures Chapter




