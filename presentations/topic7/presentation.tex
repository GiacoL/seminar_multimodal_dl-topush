% preamble ---------------------------------------------------------------
\documentclass[xcolor=dvipsnames]{beamer}

% meta data --------------------------------------------------------------
\title[Text supporting CV models]{Seminar: Multimodal Deep Learning}
\subtitle{Topic 7: Text supporting CV models}
\author[Max Schneider]{Author: Maximilian Schneider\\
        Supervisor: Jann Goschenhofer}
\institute[]{Department Of Statistics\\
             Ludwig-Maximilians-Universit√§t\\
             \vspace{0.5cm}
             \includegraphics[scale=0.02]{figures/Sigillum}%
            }
\date{\today}

% theme
\usetheme{Madrid}
\usecolortheme{dove}
\beamertemplatenavigationsymbolsempty  % no navigations symbols
\setbeamertemplate{section in toc}{\hspace*{1em}\textcolor{OliveGreen}{\inserttocsectionnumber}~\inserttocsection\par}
\setbeamertemplate{subsection in toc}{\hspace*{2em}\textcolor{OliveGreen}{\inserttocsectionnumber.\inserttocsubsectionnumber}~\inserttocsubsection\par}  % numbered table of contents
\setbeamercovered{transparent}
\usefonttheme[onlymath]{serif}  % better math font

% packages ---------------------------------------------------------------
% text encoding
\usepackage[utf8]{inputenc}

% better interaction with pdf (strg + f)
\usepackage[T1]{fontenc}

% comment, quote environment (load csquotes before babel)
\usepackage{comment, csquotes}

% language, line breaks
\usepackage[english]{babel}

% date format
\usepackage[datesep=.]{datetime2}
\DTMsetdatestyle{ddmmyyyy}

% bibliography
\usepackage[style=authoryear, backend=biber, sorting=nty, uniquename=false]{biblatex}
\addbibresource{../../book.bib}
% \addbibresource{sources.bib}
\renewcommand*{\bibsetup}{%
  \interlinepenalty=10000\relax % default is 5000
  \widowpenalty=10000\relax
  \clubpenalty=10000\relax
  \raggedbottom
  \frenchspacing
  \biburlsetup}

% load external figures, embed functioning links
\usepackage{graphicx, hyperref}

% sub-figures
\usepackage{caption, subcaption}

% math symbols
\usepackage{amsmath, amssymb, bm}

\begin{document} %--------------------------------------------------------

{
\setbeamertemplate{footline}{}
\begin{frame}
  \titlepage
\end{frame}
}
\addtocounter{framenumber}{-1}

\section{Introduction} %--------------------------------------------------

\begin{frame}{Introduction: Scale}
  \blockquote{
    The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.
    \ldots
    Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available.
    Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.
    \ldots
  }
\end{frame}

\begin{frame}{Introduction: Scale}
  \blockquote[\cite{sutton2019bitterlesson}]{
    \ldots
    One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great.
    The two methods that seem to scale arbitrarily in this way are search and learning.
  }
\end{frame}

\section{Concepts} %------------------------------------------------------
\begin{frame}{Contents}
  \tableofcontents[sections={1-2}, currentsection]
  \tableofcontents[sections={3-6}]
\end{frame}

\subsection{Web-scale data}
\begin{frame}{Concepts: Web-scale data}
  \begin{itemize}
    \item Possible through natural language supervision
    \item No labor intensive manual labeling
    \item Large datasets
      \begin{itemize}
        \item 400 million \parencite[CLIP;][]{radford2021learning}
        \item 900 million \parencite[Florence;][]{yuan2021florence}
        \item 1.8 billion \parencite[ALIGN;][]{jia2021scaling}
      \end{itemize}
    \item Pre-processing needed, resulting in arbitrary choices
    \item Social biases are reproduced
  \end{itemize}
\end{frame}

\subsection{Contrastive objective}
\begin{frame}{Concepts: Contrastive objective}
  \begin{itemize}
    \item Data efficient
    % Formula here
  \end{itemize}
\end{frame}

\begin{frame}{Concepts: Contrastive objective}
  \begin{figure}[ht]
    \centering
    % One picture for visualization of contrastive loss (matrix stuff)
    \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\linewidth]{Figures/spline_basis_cr_function}
      \caption{One basis function}
      \label{fig:splines_cr_fct}
    \end{subfigure}%
    % One picutre for visualization of effectiveness
    \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\linewidth]{Figures/spline_basis_cr_unrestricted}
      \caption{Unrestricted spline basis}
      \label{fig:splines_cr_unres_basis}
    \end{subfigure}
    \caption{Steps of fitting a cubic regression spline.}
    \label{fig:splines_cr}
  \end{figure}
\end{frame}

\subsection{Zero shooting and foundation models}
\subsection{Connecting image representations to language}

\section{Architectures} %-------------------------------------------------

\subsection{CLIP}

% Also the chosen transformer using architectures allow for large sample sizes, agressive parallelization

\subsection{ALIGN}
\subsection{Florence}

\section{Performance comparison} %----------------------------------------

\section{Resources} %-----------------------------------------------------

\section{Sources} %-------------------------------------------------------
\begin{frame}[allowframebreaks]{Sources}
  \printbibliography[heading=bibnumbered, title = Bibliography]
\end{frame}

\end{document} %----------------------------------------------------------
