#  Text + Image

*Author: Steffen Jauch-Walser *

*Supervisor: *

Data2vec: A general framework for self-supervised Learning

There have been many advances made in machine learning over the past years. However, there are two caveats. One model follows the next in short sequence. The overabundance of different models makes it hard to keep track. More importantly, however, it is often unclear whether advances in a particular field, for example with a specific type of input data, will carry over to another setting. On top of that, any model that requires labelled data inherently suffers from capacity constraints. Typically, models are trained on a handful of well-known data sets which have been created with great effort. How would a perfect model look like? Ideally, we would want to find that one general model to rule them all, a model structure that works with different inputs, little oversight and continuously adapts itself similar to the human brain. \
  Although the human brain has been used as an inspiration for neural networks, mimicking brain structures is not the aim of machine learning nor should it be. Human learning is nevertheless useful in defining potential goals. There is more to machine learning than simply finding better predictions. Making models interpretable, making models independent of human capacity constraints, making models which work across different modalities and with potentially unknown inputs and creating model structures that are reusable as well as understandable are valuable aims, too. \
  In their paper, data2vec (citation), data scientists at Meta, formerly facebook, developed an architecture that addresses some of those goals. Their algorithmic structure is able to work with either text, image or speech. On top of that, the model is self supervised with a teacher-student relationship which reduces the need  for human labelling. It is not a universal model in the sense that it works with any input, nor is it even a general model in the sense that the algorithm is exactly the same for each modality. However, the overall model structure remains the same for either text, speech or image input data, while only the specific encoding, normalization and masking strategies are modality-specific. In that regard, it is a step towards a more general way of dealing with different modalities and it is very effective at doing so given the benchmark results on typical data sets. 

— add benchmarks here?

In the following, we’ll take a closer look at the data2vec framework. According to the authors, the core idea of the framework is to “predict latent representations of the full input data based on a masked view of the input in a self-distillation set-up using a standard Transformer architecture” (citation). 

— add paragraph about transformers here?

More specifically, the framework is self-supervised, i.e. its core building blocks are a student and a teacher model whereby the teacher only differs in that it uses weights which are “an exponentially decaying average of the student model”. The transformer architecture itself follows an off-the-shelf network proposed by Vaswani et all, 2017 (citation). The exact setup can been in the following picture:

— add picture 

It is important to note that while the teach model is presented the full input data, the student model only obtains a masked, i.e a partial, view of the input data. Given that masked input, the task of the student model is to predict the latent representations created by the teach model. Specifically, the output of the top K blocks of the teacher model as highlighted in the graphic. It is notable that those latent representations are created from the complete input data and hence they are contextualized, which is not the case if you use visual tokens or pixels isolated to a current patch.




Diving deeper in to the model structure, the authors use the following loss function:

— L = either L1 regularized or L2 depending on a parameter beta 
The advantage of that particular loss function is that it is less sensitive to outlier, but one has to finetune beta. 

As far as the parameterization of the teacher model weights are concerned, 
they are implemented as 

— show equation

In essence, this means that the teacher model update more frequently at the start of the training process when the model is still random and slower towards the end when meaningful weights have been learned. Aside from that, the teacher and student model are identical. Parameters of the feature encoder and positional encoder are shared between both models. 

As far as the targets are concerned, they are constructed based on the outcome of the top K blocks of the teacher model as mentioned above. Specifically, a normalization is applied to each block and then outcomes are averaged across K blocks. The authors mention that averaging turned out to be more efficient than predicting each block separately at similar prediction rates. Normalization is important to help prevent model collapse as well as the domination of certain layers. As mentioned before, the normalization step is one of the parts of the model that is modality specific. For speech representations, instanced normalization is used. For natural language processing (NLP) and computer vision (CV), parameterless layer-normalization is used.

— potentially explain more about normalization and variance-invariance-covariance normalization that was not used. 

The other modality specific parts of the model are the encoding and the masking strategies. 

Computer Vision:

* 224x224  pixel as patches of 16x16 pixels
* each patch linearly transformed and a sequence of 196 representations is input into
* following BEit (Bao et al, 2021). 

* —show picture of paper and explanation

* masking blocks of multiple adjacent patches where each block contains at least 16 patches o * with random aspect ratio
* masking 60% of patches instead of 40%. apparently more accurate
* pre-trained Vit-B and Vit-L for 800 epochs

Speech:

* fairseq implementation (Ott et al, 2019)
* 16 kHz input
* feature encoder containing several temporal convolutions with   512 channels, strides (5,2,2,2,2,2,2) and kernal widths (10,3,3,3,2,2)
* as a result: 50Hz output with stride of 20ms between samples and receptive field of 400 input samples or 25ms of audio, raw waveform input to the encoder normalized to zero mean and unit variance
* masking identical to (Baevski et al 2020b): samples p=0.065 of all time steps and mask the subsequent ten timesteps -> approx 50% of timesteps masked





NLP:

* input tokenized using byte pair encoding. 50k types
* BERT masking strategy appliedto 15% uniformly selcted tokens
* also considered, wave2vec strategy to mask a span of four tokens




Other models:
* NLP Bert 
* Dino, Byol 
* HuBert  
* wave2vec
----
* PeCo
* flamingo 

How do they relate to data2vec? Create tables?


Findings:
CV:

* ImageNet 1K
* top1 accuracy. data2vec outperforms Vit-L and Vit-B in single model setting.
* accuracy similar to PeCo (multiple models setting)

Speech Processing:

* Librispeech 960 (audiobooks in engl, clear speech)
* improvements particularly in the section with shorter training (10min - 1h)

NLP:

* Books Corpus and English Wikipedia data. GLUE benchmark 
* first successful pre-trained nlp model not sureing discrete units as training target
* outperforms roberta baseline

Generally, best accuracy at around 10-12 layers. 
The model performs best when teacher is given full input.

What do the findings mean for the future of the field? 
The authors succeed in designing a single learning mechanism for different modalities. As a caveat, they still use modality specific encoding and masking strategies, but input data is also quite different. Is it possible to go beyond that? One of the main advances of the framework is the use of contextualized training targets through the use of the teach self-attention mechanism.
